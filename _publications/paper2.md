---
title: "Could We Have Had Better Multilingual LLMs If English Was Not the Central Language?"
collection: publications
permalink: /publication/central-language
excerpt: "Large Language Models (LLMs) demonstrate strong machine translation capabilities on languages they are trained on. However, the impact of factors beyond training data size on translation performance remains a topic of debate, especially concerning languages not directly encountered during training. Our study delves into Llama2's translation capabilities. By modeling a linear relationship between linguistic feature distances and machine translation scores, we ask ourselves if there are potentially better central languages for LLMs other than English."
date: 2024 May
venue: 'LREC-Coling 2024'
paperurl: 'TBD'
citation: 'TBD'
---
Large Language Models (LLMs) demonstrate strong machine translation capabilities on languages they are trained on. However, the impact of factors beyond training data size on translation performance remains a topic of debate, especially concerning languages not directly encountered during training. Our study delves into Llama2's translation capabilities. By modeling a linear relationship between linguistic feature distances and machine translation scores, we ask ourselves if there are potentially better central languages for LLMs other than English.

Accepted at the Second International TDLE Workshop at the LREC-COLING 2024 Conference in Turin.